{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectiminimax\n",
    "\n",
    "Der Vollständigkeits halber der ganze Expectiminimax Algorithmus. <br>\n",
    "Während 1-ply, 2-ply und 3-ply nur den ersten, die ersten beiden, bzw. ersten drei Schritte von Expectiminimax ausgeführt haben, kann man alle mit dem Expectiminmax Algorithmus zusammenfassen. Das erlaubt einem eine saubere Notation und kann (mit einem ausreichend starken Rechner und genug Geduld) eventuell noch tiefer suchen!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Player import ValuePlayer\n",
    "\n",
    "class ExpectiminimaxValuePlayer(ValuePlayer):\n",
    "\n",
    "    # Konstruktor braucht einen Parameter für die maximal Suchtiefe\n",
    "    # 0 = 1-ply, 1= 2-ply, 2 = 3-ply, usw.\n",
    "    def __init__(self, player, valuefunction, max_depth):\n",
    "        ValuePlayer.__init__(self, player, valuefunction)\n",
    "        self.max_depth = max_depth\n",
    "    \n",
    "    def get_action(self, actions, game):\n",
    "        # Spielstatus speichern\n",
    "        old_state = game.get_state()\n",
    "        # Variablen initialisieren\n",
    "        best_value = -1\n",
    "        best_action = None\n",
    "        # Alle Züge durchsuchen\n",
    "        for a in actions:\n",
    "            # Zug ausführen\n",
    "            game.execute_moves(a, self.player)\n",
    "            # Spielstatus bewerten\n",
    "            value = self.expectiminimax(game, 0)\n",
    "            # Besten merken\n",
    "            if value > best_value:\n",
    "                best_value = value\n",
    "                best_action = a\n",
    "            # Spiel zurücksetzen\n",
    "            game.reset_to_state(old_state)\n",
    "        return best_action\n",
    "        \n",
    "    def expectiminimax(self, game, depth):\n",
    "        # Blatt in unserem Baum\n",
    "        if depth == self.max_depth:\n",
    "            return self.value(game, self.player)\n",
    "        else:\n",
    "            # Alle möglichen Würfe betrachten\n",
    "            all_rolls = [(a,b) for a in range(1,7) for b in range(a,7)]\n",
    "            value = 0\n",
    "            for roll in all_rolls:\n",
    "                # Wahrscheinlichkeiten von jedem Wurf\n",
    "                probability = 1/18 if roll[0] != roll[1] else 1/36\n",
    "                state = game.get_state()\n",
    "                # Min-Knoten\n",
    "                if depth % 2 == 0:\n",
    "                    moves = game.get_moves(roll, game.get_opponent(self.player))\n",
    "                    temp_val = 1\n",
    "                    for move in moves:\n",
    "                        game.execute_moves(move, game.get_opponent(self.player))\n",
    "                        # Bewertet wird aber aus unserer Perspektive\n",
    "                        v = self.expectiminimax(game, depth + 1)\n",
    "                        if v < temp_val:\n",
    "                            temp_val = v\n",
    "                # Max-Knoten\n",
    "                else:\n",
    "                    moves = game.get_moves(roll, self.player)\n",
    "                    temp_val = 0\n",
    "                    for move in moves:\n",
    "                        game.execute_moves(move, self.player)\n",
    "                        # Bewertet wird aber aus unserer Perspektive\n",
    "                        v = self.expectiminimax(game, depth + 1)\n",
    "                        if v > temp_val:\n",
    "                            temp_val = v\n",
    "                # Spiel zurücksetzen    \n",
    "                game.reset_to_state(state)\n",
    "                # Wert gewichtet addieren\n",
    "                value += probability * temp_val\n",
    "            return value\n",
    "    \n",
    "    def get_name(self):\n",
    "        return \"ExpectiminimaxValuePlayer [\" + self.value.__name__ + \"]\"\n",
    "    \n",
    "\n",
    "class ExpectiminimaxModelPlayer(ExpectiminimaxValuePlayer):\n",
    "    \n",
    "    def __init__(self, player, model, depth):\n",
    "        ExpectiminimaxValuePlayer.__init__(self, player, self.get_value, depth)\n",
    "        self.model = model\n",
    "        \n",
    "    def get_value(self, game, player):\n",
    "        features = game.extractFeatures(player)\n",
    "        v = self.model.get_output(features)\n",
    "        v = 1 - v if self.player == game.players[0] else v\n",
    "        return v\n",
    "    \n",
    "    def get_name(self):\n",
    "        return \"EMinMaxModelPlayer [\" + self.model.get_name() +\"]\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring checkpoint: checkpoints/TD-Gammon/checkpoint.ckpt-1593683\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/TD-Gammon/checkpoint.ckpt-1593683\n",
      "[Game 0] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 0:1 of 1 games (0.00%)\n",
      "[Game 1] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 1:1 of 2 games (50.00%)\n",
      "[Game 2] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 1:2 of 3 games (33.33%)\n",
      "[Game 3] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 2:2 of 4 games (50.00%)\n",
      "[Game 4] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 3:2 of 5 games (60.00%)\n",
      "[Game 5] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 4:2 of 6 games (66.67%)\n",
      "[Game 6] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 4:3 of 7 games (57.14%)\n",
      "[Game 7] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 4:4 of 8 games (50.00%)\n",
      "[Game 8] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 4:5 of 9 games (44.44%)\n",
      "[Game 9] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 4:6 of 10 games (40.00%)\n",
      "[Game 10] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 4:7 of 11 games (36.36%)\n",
      "[Game 11] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 4:8 of 12 games (33.33%)\n",
      "[Game 12] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 4:9 of 13 games (30.77%)\n",
      "[Game 13] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 4:10 of 14 games (28.57%)\n",
      "[Game 14] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 5:10 of 15 games (33.33%)\n",
      "[Game 15] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 5:11 of 16 games (31.25%)\n",
      "[Game 16] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 5:12 of 17 games (29.41%)\n",
      "[Game 17] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 5:13 of 18 games (27.78%)\n",
      "[Game 18] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 5:14 of 19 games (26.32%)\n",
      "[Game 19] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 6:14 of 20 games (30.00%)\n",
      "[Game 20] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 6:15 of 21 games (28.57%)\n",
      "[Game 21] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 7:15 of 22 games (31.82%)\n",
      "[Game 22] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 7:16 of 23 games (30.43%)\n",
      "[Game 23] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 8:16 of 24 games (33.33%)\n",
      "[Game 24] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 8:17 of 25 games (32.00%)\n",
      "[Game 25] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 8:18 of 26 games (30.77%)\n",
      "[Game 26] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 8:19 of 27 games (29.63%)\n",
      "[Game 27] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 9:19 of 28 games (32.14%)\n",
      "[Game 28] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 10:19 of 29 games (34.48%)\n",
      "[Game 29] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 11:19 of 30 games (36.67%)\n",
      "[Game 30] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 11:20 of 31 games (35.48%)\n",
      "[Game 31] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 12:20 of 32 games (37.50%)\n",
      "[Game 32] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 12:21 of 33 games (36.36%)\n",
      "[Game 33] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 13:21 of 34 games (38.24%)\n",
      "[Game 34] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 13:22 of 35 games (37.14%)\n",
      "[Game 35] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 13:23 of 36 games (36.11%)\n",
      "[Game 36] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 13:24 of 37 games (35.14%)\n",
      "[Game 37] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 13:25 of 38 games (34.21%)\n",
      "[Game 38] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 13:26 of 39 games (33.33%)\n",
      "[Game 39] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 14:26 of 40 games (35.00%)\n",
      "[Game 40] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 15:26 of 41 games (36.59%)\n",
      "[Game 41] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 15:27 of 42 games (35.71%)\n",
      "[Game 42] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 15:28 of 43 games (34.88%)\n",
      "[Game 43] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 15:29 of 44 games (34.09%)\n",
      "[Game 44] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 15:30 of 45 games (33.33%)\n",
      "[Game 45] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 15:31 of 46 games (32.61%)\n",
      "[Game 46] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 15:32 of 47 games (31.91%)\n",
      "[Game 47] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 16:32 of 48 games (33.33%)\n",
      "[Game 48] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 16:33 of 49 games (32.65%)\n",
      "[Game 49] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 17:33 of 50 games (34.00%)\n",
      "[Game 50] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 17:34 of 51 games (33.33%)\n",
      "[Game 51] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 17:35 of 52 games (32.69%)\n",
      "[Game 52] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 18:35 of 53 games (33.96%)\n",
      "[Game 53] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 18:36 of 54 games (33.33%)\n",
      "[Game 54] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 18:37 of 55 games (32.73%)\n",
      "[Game 55] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 19:37 of 56 games (33.93%)\n",
      "[Game 56] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 19:38 of 57 games (33.33%)\n",
      "[Game 57] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 20:38 of 58 games (34.48%)\n",
      "[Game 58] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 20:39 of 59 games (33.90%)\n",
      "[Game 59] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 20:40 of 60 games (33.33%)\n",
      "[Game 60] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 20:41 of 61 games (32.79%)\n",
      "[Game 61] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 21:41 of 62 games (33.87%)\n",
      "[Game 62] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 22:41 of 63 games (34.92%)\n",
      "[Game 63] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 23:41 of 64 games (35.94%)\n",
      "[Game 64] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 23:42 of 65 games (35.38%)\n",
      "[Game 65] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 24:42 of 66 games (36.36%)\n",
      "[Game 66] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 24:43 of 67 games (35.82%)\n",
      "[Game 67] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 24:44 of 68 games (35.29%)\n",
      "[Game 68] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 25:44 of 69 games (36.23%)\n",
      "[Game 69] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 25:45 of 70 games (35.71%)\n",
      "[Game 70] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 25:46 of 71 games (35.21%)\n",
      "[Game 71] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 26:46 of 72 games (36.11%)\n",
      "[Game 72] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 26:47 of 73 games (35.62%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Game 73] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 27:47 of 74 games (36.49%)\n",
      "[Game 74] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 27:48 of 75 games (36.00%)\n",
      "[Game 75] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 27:49 of 76 games (35.53%)\n",
      "[Game 76] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 27:50 of 77 games (35.06%)\n",
      "[Game 77] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 27:51 of 78 games (34.62%)\n",
      "[Game 78] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 27:52 of 79 games (34.18%)\n",
      "[Game 79] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 28:52 of 80 games (35.00%)\n",
      "[Game 80] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 29:52 of 81 games (35.80%)\n",
      "[Game 81] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 29:53 of 82 games (35.37%)\n",
      "[Game 82] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 29:54 of 83 games (34.94%)\n",
      "[Game 83] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 29:55 of 84 games (34.52%)\n",
      "[Game 84] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 29:56 of 85 games (34.12%)\n",
      "[Game 85] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 29:57 of 86 games (33.72%)\n",
      "[Game 86] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 29:58 of 87 games (33.33%)\n",
      "[Game 87] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 29:59 of 88 games (32.95%)\n",
      "[Game 88] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 30:59 of 89 games (33.71%)\n",
      "[Game 89] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 31:59 of 90 games (34.44%)\n",
      "[Game 90] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 31:60 of 91 games (34.07%)\n",
      "[Game 91] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 31:61 of 92 games (33.70%)\n",
      "[Game 92] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 31:62 of 93 games (33.33%)\n",
      "[Game 93] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 31:63 of 94 games (32.98%)\n",
      "[Game 94] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 32:63 of 95 games (33.68%)\n",
      "[Game 95] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 32:64 of 96 games (33.33%)\n",
      "[Game 96] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 33:64 of 97 games (34.02%)\n",
      "[Game 97] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 34:64 of 98 games (34.69%)\n",
      "[Game 98] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 35:64 of 99 games (35.35%)\n",
      "[Game 99] ModelPlayer [TD-Gammon] (black) vs EMinMaxModelPlayer [TD-Gammon] (white) 35:65 of 100 games (35.00%)\n"
     ]
    }
   ],
   "source": [
    "import Player\n",
    "from NeuralNetModel import TDGammonModel\n",
    "import tensorflow as tf\n",
    "\n",
    "graph = tf.Graph()\n",
    "sess = tf.Session(graph=graph)\n",
    "with sess.as_default(), graph.as_default():\n",
    "    model = TDGammonModel(sess, restore=True)\n",
    "    model.test(games = 100, enemyPlayer = ExpectiminimaxModelPlayer('white', model, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spiel 0 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 1 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 2 von 100 geht an ValuePlayer [blocker] ( black )\n",
      "Spiel 3 von 100 geht an ValuePlayer [blocker] ( black )\n",
      "Spiel 4 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 5 von 100 geht an ValuePlayer [blocker] ( black )\n",
      "Spiel 6 von 100 geht an ValuePlayer [blocker] ( black )\n",
      "Spiel 7 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 8 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 9 von 100 geht an ValuePlayer [blocker] ( black )\n",
      "Spiel 10 von 100 geht an ValuePlayer [blocker] ( black )\n",
      "Spiel 11 von 100 geht an ValuePlayer [blocker] ( black )\n",
      "Spiel 12 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 13 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 14 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 15 von 100 geht an ValuePlayer [blocker] ( black )\n",
      "Spiel 16 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 17 von 100 geht an ValuePlayer [blocker] ( black )\n",
      "Spiel 18 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 19 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 20 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 21 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 22 von 100 geht an ValuePlayer [blocker] ( black )\n",
      "Spiel 23 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 24 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 25 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 26 von 100 geht an ValuePlayer [blocker] ( black )\n",
      "Spiel 27 von 100 geht an ValuePlayer [blocker] ( black )\n",
      "Spiel 28 von 100 geht an ValuePlayer [blocker] ( black )\n",
      "Spiel 29 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 30 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 31 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 32 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 33 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 34 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 35 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 36 von 100 geht an ValuePlayer [blocker] ( black )\n",
      "Spiel 37 von 100 geht an ValuePlayer [blocker] ( black )\n",
      "Spiel 38 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 39 von 100 geht an ValuePlayer [blocker] ( black )\n",
      "Spiel 40 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 41 von 100 geht an ValuePlayer [blocker] ( black )\n",
      "Spiel 42 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 43 von 100 geht an ValuePlayer [blocker] ( black )\n",
      "Spiel 44 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 45 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 46 von 100 geht an ValuePlayer [blocker] ( black )\n",
      "Spiel 47 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 48 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 49 von 100 geht an ValuePlayer [blocker] ( black )\n",
      "Spiel 50 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 51 von 100 geht an ValuePlayer [blocker] ( black )\n",
      "Spiel 52 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 53 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 54 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 55 von 100 geht an ValuePlayer [blocker] ( black )\n",
      "Spiel 56 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 57 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 58 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 59 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 60 von 100 geht an ValuePlayer [blocker] ( black )\n",
      "Spiel 61 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 62 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 63 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 64 von 100 geht an ValuePlayer [blocker] ( black )\n",
      "Spiel 65 von 100 geht an ValuePlayer [blocker] ( black )\n",
      "Spiel 66 von 100 geht an ValuePlayer [blocker] ( black )\n",
      "Spiel 67 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 68 von 100 geht an ValuePlayer [blocker] ( black )\n",
      "Spiel 69 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 70 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 71 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 72 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 73 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 74 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 75 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 76 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 77 von 100 geht an ValuePlayer [blocker] ( black )\n",
      "Spiel 78 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 79 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 80 von 100 geht an ValuePlayer [blocker] ( black )\n",
      "Spiel 81 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 82 von 100 geht an ValuePlayer [blocker] ( black )\n",
      "Spiel 83 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 84 von 100 geht an ValuePlayer [blocker] ( black )\n",
      "Spiel 85 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 86 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 87 von 100 geht an ValuePlayer [blocker] ( black )\n",
      "Spiel 88 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 89 von 100 geht an ValuePlayer [blocker] ( black )\n",
      "Spiel 90 von 100 geht an ValuePlayer [blocker] ( black )\n",
      "Spiel 91 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 92 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 93 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 94 von 100 geht an ValuePlayer [blocker] ( black )\n",
      "Spiel 95 von 100 geht an ValuePlayer [blocker] ( black )\n",
      "Spiel 96 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 97 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 98 von 100 geht an ExpectiminimaxValuePlayer [blocker] ( white )\n",
      "Spiel 99 von 100 geht an ValuePlayer [blocker] ( black )\n",
      "\n",
      "{'white': 63, 'black': 37}\n",
      "100 Spiele in  617.9951107501984 Sekunden\n"
     ]
    }
   ],
   "source": [
    "import Player\n",
    "import PlayerTest\n",
    "\n",
    "players = [Player.ValuePlayer('black', Player.blocker), ExpectiminimaxValuePlayer('white', Player.blocker, 1)]\n",
    "PlayerTest.test(players, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring checkpoint: checkpoints/TD-Gammon/checkpoint.ckpt-1593683\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/TD-Gammon/checkpoint.ckpt-1593683\n",
      "Spiel 0 von 10 geht an ModelPlayer [TD-Gammon] ( black )\n",
      "Spiel 1 von 10 geht an ModelPlayer [TD-Gammon] ( black )\n",
      "Spiel 2 von 10 geht an EMinMaxModelPlayer [TD-Gammon] ( white )\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-277b041c5562>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTDGammonModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrestore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mplayers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mPlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mModelPlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'black'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExpectiminimaxModelPlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'white'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mPlayerTest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplayers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Desktop\\Uni\\Machine Learning\\TD-Gammon-Implementation\\PlayerTest.py\u001b[0m in \u001b[0;36mtest\u001b[1;34m(players, games, debug)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mgame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mwinner\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplayers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mwins\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mwinner\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mwin_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mwinner\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Uni\\Machine Learning\\TD-Gammon-Implementation\\CythonBackgammon.pyx\u001b[0m in \u001b[0;36mCythonBackgammon.Game.play\u001b[1;34m()\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_winner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[1;31m#Zug ausführen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplayer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mplayer_num\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplayer_num\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m             \u001b[1;31m#Der andere Spieler ist dran\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[0mplayer_num\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mplayer_num\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Uni\\Machine Learning\\TD-Gammon-Implementation\\CythonBackgammon.pyx\u001b[0m in \u001b[0;36mCythonBackgammon.Game.next_step\u001b[1;34m()\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[0mmoves\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_moves\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroll\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mplayer_num\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[1;31m#Spieler fragen welche der Züge er gerne ausführen möchte\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m         \u001b[0mmove\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmoves\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mmoves\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    139\u001b[0m         \u001b[1;31m#Zug ausführen falls es möglich ist\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmove\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Uni\\Machine Learning\\TD-Gammon-Implementation\\Player.py\u001b[0m in \u001b[0;36mget_action\u001b[1;34m(self, actions, game)\u001b[0m\n\u001b[0;32m    260\u001b[0m             \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute_moves\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m             \u001b[1;31m# Spielstatus bewerten\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpectiminimax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m             \u001b[1;31m# Besten merken\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbest_value\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Uni\\Machine Learning\\TD-Gammon-Implementation\\Player.py\u001b[0m in \u001b[0;36mexpectiminimax\u001b[1;34m(self, game, depth)\u001b[0m\n\u001b[0;32m    288\u001b[0m                         \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute_moves\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmove\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_opponent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m                         \u001b[1;31m# Bewertet wird aber aus unserer Perspektive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 290\u001b[1;33m                         \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpectiminimax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    291\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mtemp_val\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m                             \u001b[0mtemp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Uni\\Machine Learning\\TD-Gammon-Implementation\\Player.py\u001b[0m in \u001b[0;36mexpectiminimax\u001b[1;34m(self, game, depth)\u001b[0m\n\u001b[0;32m    298\u001b[0m                         \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute_moves\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmove\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m                         \u001b[1;31m# Bewertet wird aber aus unserer Perspektive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                         \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpectiminimax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m                         \u001b[1;32mif\u001b[0m \u001b[0mv\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mtemp_val\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                             \u001b[0mtemp_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Uni\\Machine Learning\\TD-Gammon-Implementation\\Player.py\u001b[0m in \u001b[0;36mexpectiminimax\u001b[1;34m(self, game, depth)\u001b[0m\n\u001b[0;32m    272\u001b[0m         \u001b[1;31m# Blatt in unserem Baum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdepth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m             \u001b[1;31m# Alle möglichen Würfe betrachten\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Uni\\Machine Learning\\TD-Gammon-Implementation\\Player.py\u001b[0m in \u001b[0;36mget_value\u001b[1;34m(self, game, player)\u001b[0m\n\u001b[0;32m    319\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextractFeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m         \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m         \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplayer\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Desktop\\Uni\\Machine Learning\\TD-Gammon-Implementation\\NeuralNetModel.py\u001b[0m in \u001b[0;36mget_output\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;31m#Erzeugt einen Outpt für die gegebenen features x\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mV\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;31m#Testet das Modell gegen den angegebenen enemyAgent\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tobi\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    903\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 905\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    906\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tobi\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1140\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1141\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tobi\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1321\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tobi\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tobi\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1312\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\tobi\\anaconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[0;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1420\u001b[1;33m             status, run_metadata)\n\u001b[0m\u001b[0;32m   1421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1422\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import Player\n",
    "import PlayerTest\n",
    "from NeuralNetModel import TDGammonModel\n",
    "import tensorflow as tf\n",
    "\n",
    "graph = tf.Graph()\n",
    "sess = tf.Session(graph=graph)\n",
    "with sess.as_default(), graph.as_default():\n",
    "    model = TDGammonModel(sess, restore=True)\n",
    "    players = [Player.ModelPlayer('black', model), Player.ExpectiminimaxModelPlayer('white', model, 2)]\n",
    "    PlayerTest.test(players, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diese 3 Spiele haben 24 Stunden gedauert...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
