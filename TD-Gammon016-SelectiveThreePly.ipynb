{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selective Three-Ply\n",
    "\n",
    "\"To save computer time, the second ply of search was conducted only for candidate moves that were ranked highly after\n",
    "the first ply, about four or five moves on average. Two-ply search affected only the moves selected; the\n",
    "learning process proceeded exactly as before. The finnal versions of the program, TD-Gammon 3.0 and 3.1, used 160 hidden units and a selective three-ply search\"\n",
    "\n",
    "__Reinforcement Learning: An Introduction, Sutton & Barto, 2017__\n",
    "\n",
    "\n",
    "Um 3-ply erträglich zu machen wird die Anzahl der zu durchsuchenden Züge auf der höchsten Ebene auf ein kleine Zahl begrenzt. Das reduziert zwar eventuell das Potenzial von 3-ply sollte aber auch Ergebnisse in absehbarer Zeit liefern können.\n",
    "\n",
    "### SelectiveTPVPlayer\n",
    "Aus offensichtlichen Gründen abgekürzt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "from Player import ThreePlyValuePlayer\n",
    "\n",
    "class SelectiveTPVPlayer(ThreePlyValuePlayer):\n",
    "\n",
    "    # Sucht die 5 besten Züge mit 1-ply und untersucht diese dann mit 3-ply um\n",
    "    # den Rechenaufwand zu reduzieren\n",
    "    def get_action(self, actions, game):\n",
    "        # Spielstatus speichern\n",
    "        old_state = game.get_state()\n",
    "        # Variablen initialisieren\n",
    "        best_value = float(\"-inf\")\n",
    "        best_action = None\n",
    "        moves_score = []\n",
    "        # Alle Züge durchsuchen\n",
    "        for a in actions:\n",
    "            # Zug ausführen\n",
    "            game.execute_moves(a, self.player)\n",
    "            # Spielstatus bewerten\n",
    "            value = self.value(game, self.player)\n",
    "            #Zur liste hinzufügen\n",
    "            moves_score.append((a, value))\n",
    "            # Besten merken\n",
    "            if value > best_value:\n",
    "                best_value = value\n",
    "                best_action = a\n",
    "            # Spiel zurücksetzen\n",
    "            game.reset_to_state(old_state)\n",
    "        # Die besten 5 Züge weiter untersuchen und mit 2-ply/3-ply bewerten\n",
    "        moves_score = sorted(moves_score, key=lambda tup: tup[1])\n",
    "        top_five = moves_score[-5:]\n",
    "        top_five_actions = [x for (x,_) in top_five]\n",
    "        return ThreePlyValuePlayer.get_action(self, top_five_actions, game)\n",
    "    \n",
    "    def get_name(self):\n",
    "        return \"SelectiveTPVPlayer [\" + self.value.__name__ + \"]\"\n",
    "    \n",
    "class SelectiveTPMPlayer(SelectiveTPVPlayer):\n",
    "\n",
    "    def __init__(self, player, model):\n",
    "        SelectiveTPVPlayer.__init__(self, player, self.get_value)\n",
    "        self.model = model\n",
    "        \n",
    "    def get_value(self, game, player):\n",
    "        features = game.extractFeatures(player)\n",
    "        v = self.model.get_output(features)\n",
    "        v = 1 - v if self.player == game.players[0] else v\n",
    "        return v\n",
    "    \n",
    "    def get_name(self):\n",
    "        return \"SelectiveTPMPlayer [\" + self.model.get_name() +\"]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Testspiel\n",
    "\n",
    "Wie lange dauert ein Spiel?\n",
    "\n",
    "PlayerTest benutzt nun auch CythonBackgammon um das Spiel zu beschleunigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spiel 0 von 1 geht an ValuePlayer [singleton] ( black )\n",
      "\n",
      "{'white': 0, 'black': 1}\n",
      "1 Spiele in  233.90840005874634 Sekunden\n"
     ]
    }
   ],
   "source": [
    "import Player\n",
    "import PlayerTest\n",
    "\n",
    "players = [Player.ValuePlayer('black', Player.singleton), SelectiveTPVPlayer('white', Player.singleton)]\n",
    "\n",
    "PlayerTest.test(players, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__4 Minuten__ statt 25 pro Spiel sind schon deutlich besser!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spiel 0 von 10 geht an SelectiveTPVPlayer [singleton] ( white )\n",
      "Spiel 1 von 10 geht an SelectiveTPVPlayer [singleton] ( white )\n",
      "Spiel 2 von 10 geht an ValuePlayer [singleton] ( black )\n",
      "Spiel 3 von 10 geht an SelectiveTPVPlayer [singleton] ( white )\n",
      "Spiel 4 von 10 geht an ValuePlayer [singleton] ( black )\n",
      "Spiel 5 von 10 geht an SelectiveTPVPlayer [singleton] ( white )\n",
      "Spiel 6 von 10 geht an ValuePlayer [singleton] ( black )\n",
      "Spiel 7 von 10 geht an SelectiveTPVPlayer [singleton] ( white )\n",
      "Spiel 8 von 10 geht an SelectiveTPVPlayer [singleton] ( white )\n",
      "Spiel 9 von 10 geht an ValuePlayer [singleton] ( black )\n",
      "\n",
      "{'white': 6, 'black': 4}\n",
      "10 Spiele in  2401.3864953517914 Sekunden\n"
     ]
    }
   ],
   "source": [
    "PlayerTest.test(players, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Schafft es der Three-ply singleton gegen TD-Gammon80?\n",
    "\n",
    "#### TD-Gammon80 vs SelectiveTPVPlayer Singleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring checkpoint: checkpoints/TD-Gammon80/checkpoint.ckpt-1527600\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/TD-Gammon80/checkpoint.ckpt-1527600\n",
      "[Game 0] ModelPlayer [TD-Gammon] (black) vs SelectiveTPVPlayer [singleton] (white) 1:0 of 1 games (100.00%)\n",
      "[Game 1] ModelPlayer [TD-Gammon] (black) vs SelectiveTPVPlayer [singleton] (white) 2:0 of 2 games (100.00%)\n",
      "[Game 2] ModelPlayer [TD-Gammon] (black) vs SelectiveTPVPlayer [singleton] (white) 3:0 of 3 games (100.00%)\n",
      "[Game 3] ModelPlayer [TD-Gammon] (black) vs SelectiveTPVPlayer [singleton] (white) 4:0 of 4 games (100.00%)\n",
      "[Game 4] ModelPlayer [TD-Gammon] (black) vs SelectiveTPVPlayer [singleton] (white) 4:1 of 5 games (80.00%)\n",
      "[Game 5] ModelPlayer [TD-Gammon] (black) vs SelectiveTPVPlayer [singleton] (white) 5:1 of 6 games (83.33%)\n",
      "[Game 6] ModelPlayer [TD-Gammon] (black) vs SelectiveTPVPlayer [singleton] (white) 6:1 of 7 games (85.71%)\n",
      "[Game 7] ModelPlayer [TD-Gammon] (black) vs SelectiveTPVPlayer [singleton] (white) 6:2 of 8 games (75.00%)\n",
      "[Game 8] ModelPlayer [TD-Gammon] (black) vs SelectiveTPVPlayer [singleton] (white) 6:3 of 9 games (66.67%)\n",
      "[Game 9] ModelPlayer [TD-Gammon] (black) vs SelectiveTPVPlayer [singleton] (white) 7:3 of 10 games (70.00%)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from NeuralNetModel import TDGammonModel\n",
    "\n",
    "graph = tf.Graph()\n",
    "sess = tf.Session(graph=graph)\n",
    "with sess.as_default(), graph.as_default():\n",
    "    model = TDGammonModel(sess, hidden_size = 80, name = \"TD-Gammon80\", restore=True)\n",
    "    model.test(games = 10, enemyPlayer = SelectiveTPVPlayer('white', Player.singleton))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring checkpoint: checkpoints/TD-Gammon/checkpoint.ckpt-1593683\n",
      "INFO:tensorflow:Restoring parameters from checkpoints/TD-Gammon/checkpoint.ckpt-1593683\n",
      "Spiel 0 von 10 geht an SelectiveTPMPlayer [TD-Gammon] ( white )\n",
      "Spiel 1 von 10 geht an SelectiveTPMPlayer [TD-Gammon] ( white )\n",
      "Spiel 2 von 10 geht an SelectiveTPMPlayer [TD-Gammon] ( white )\n",
      "Spiel 3 von 10 geht an SelectiveTPMPlayer [TD-Gammon] ( white )\n",
      "Spiel 4 von 10 geht an ModelPlayer [TD-Gammon] ( black )\n",
      "Spiel 5 von 10 geht an SelectiveTPMPlayer [TD-Gammon] ( white )\n",
      "Spiel 6 von 10 geht an SelectiveTPMPlayer [TD-Gammon] ( white )\n",
      "Spiel 7 von 10 geht an SelectiveTPMPlayer [TD-Gammon] ( white )\n",
      "Spiel 8 von 10 geht an SelectiveTPMPlayer [TD-Gammon] ( white )\n",
      "Spiel 9 von 10 geht an SelectiveTPMPlayer [TD-Gammon] ( white )\n",
      "\n",
      "{'white': 9, 'black': 1}\n",
      "ModelPlayer [TD-Gammon] vs. SelectiveTPMPlayer [TD-Gammon] : 10.0 %\n",
      "10 Spiele in  74149.61713314056 Sekunden\n"
     ]
    }
   ],
   "source": [
    "import Player\n",
    "import PlayerTest\n",
    "import tensorflow as tf\n",
    "from NeuralNetModel import TDGammonModel\n",
    "\n",
    "graph = tf.Graph()\n",
    "sess = tf.Session(graph=graph)\n",
    "with sess.as_default(), graph.as_default():\n",
    "    model = TDGammonModel(sess, restore=True)\n",
    "    players = [Player.ModelPlayer('black', model), SelectiveTPMPlayer('white', model)]\n",
    "    PlayerTest.test(players, games=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
